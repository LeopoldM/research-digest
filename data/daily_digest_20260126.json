{
  "generated_at": "2026-01-26T14:50:51.332908",
  "period": "daily",
  "paper_count": 14,
  "papers": [
    {
      "title": "Incentive Mechanism Design for Resource Management in Satellite Networks: A Comprehensive Survey",
      "authors": [
        "Nguyen Cong Luong",
        "Zeping Sui",
        "Duc Van Le",
        "Jie Cao",
        "Bo Ma",
        "Nguyen Duc Hai",
        "Ruichen Zhang",
        "Vu Van Quang",
        "Dusit Niyato",
        "Shaohan Feng"
      ],
      "abstract": "Resource management is one of the challenges in satellite networks due to their high mobility, wide coverage, long propagation distances, and stringent constraints on energy, communication, and computation resources. Traditional resource allocation approaches rely only on hard and rigid system performance metrics. Meanwhile, incentive mechanisms, which are based on game theory and auction theory, investigate systems from the \"economic\" perspective in addition to the \"system\" perspective. Particularly, incentive mechanisms are able to take into account rationality and other behavior of human users into account, which guarantees benefits/utility of all system entities, thereby improving the scalability, adaptability, and fairness in resource allocation. This paper presents a comprehensive survey of incentive mechanism design for resource management in satellite networks. The paper covers key issues in the satellite networks, such as communication resource allocation, computation offloading, privacy and security, and coordination. We conclude with future research directions including learning-based mechanism design for satellite networks.",
      "url": "https://arxiv.org/abs/2601.03757v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03757v1",
      "source": "arxiv",
      "source_id": "2601.03757v1",
      "published_date": "2026-01-07",
      "categories": [
        "cs.NI",
        "cs.GT"
      ],
      "relevance_score": 15.555555555555555,
      "summary": "This paper surveys incentive mechanism design approaches for resource management in satellite networks, examining how game theory and auction theory can address allocation challenges beyond traditional system-performance metrics. The methodology is a comprehensive literature review covering communication resource allocation, computation offloading, privacy/security, and coordination in satellite systems. The key contribution is demonstrating how economic-based incentive mechanisms can improve scalability, adaptability, and fairness by incorporating user rationality and behavior into satellite network resource management decisions."
    },
    {
      "title": "The Suicide Region: Option Games and the Race to Artificial General Intelligence",
      "authors": [
        "David Tan"
      ],
      "abstract": "Standard real options theory predicts delay in exercising the option to invest or deploy when extreme asset volatility or technological uncertainty are present. However, in the current race to develop artificial general intelligence (AGI), sovereign actors are exhibiting behaviors contrary to theoretical predictions: the US and China are accelerating AI investment despite acknowledging the potential for catastrophic failure from AGI misalignment. We resolve this puzzle by formalizing the AGI race as a continuous-time preemption game with endogenous existential risk. In our model, the cost of failure is no longer bounded only by the sunk cost of investment (I), but rather a systemic ruin parameter (D) that is correlated with development velocity and shared globally. As the disutility of catastrophe is embedded in both players' payoffs, the risk term mathematically cancels out of the equilibrium indifference condition. This creates a \"suicide region\" in the investment space where competitive pressures force rational agents to deploy AGI systems early, despite a negative risk-adjusted net present value. Furthermore, we show that \"warning shots\" (sub-existential disasters) will fail to deter AGI acceleration, as the winner-takes-all nature of the race remains intact. The race can only be halted if the cost of ruin is internalized, making safety research a prerequisite for economic viability. We derive the critical private liability threshold required to restore the option value of waiting and propose mechanism design interventions that can better ensure safe AGI research and socially responsible deployment.",
      "url": "https://arxiv.org/abs/2512.07526v1",
      "pdf_url": "https://arxiv.org/pdf/2512.07526v1",
      "source": "arxiv",
      "source_id": "2512.07526v1",
      "published_date": "2025-12-08",
      "categories": [
        "q-fin.RM",
        "econ.GN",
        "q-fin.GN"
      ],
      "relevance_score": 11.11111111111111,
      "summary": "This paper examines why the US and China are accelerating AI investment despite catastrophic AGI risks, contrary to standard real options theory that predicts delay under extreme uncertainty. Using a continuous-time preemption game with endogenous existential risk, the authors show that when catastrophic failure costs are shared globally, the risk term cancels out of players' equilibrium conditions, creating a \"suicide region\" where rational actors deploy AGI early despite negative risk-adjusted NPV. The key contribution is demonstrating that competitive pressures can force premature deployment of potentially catastrophic technologies, and that only internalizing ruin costs through private liability can restore incentives for delay and safety research."
    },
    {
      "title": "Sharing with Frictions: Limited Transfers and Costly Inspections",
      "authors": [
        "Federico Bobbio",
        "Randall A. Berry",
        "Michael L. Honig",
        "Thanh Nguyen",
        "Vijay G. Subramanian",
        "Rakesh V. Vohra"
      ],
      "abstract": "The radio spectrum suitable for commercial wireless services is limited. A portion of the radio spectrum has been reserved for institutions using it for non-commercial purposes such as federal agencies, defense, public safety bodies and scientific institutions. In order to operate efficiently, these incumbents need clean spectrum access. However, commercial users also want access, and granting them access may materially interfere with the existing activity of the incumbents. Conventional market based mechanisms for allocating scarce resources in this context are problematic. Allowing direct monetary transfers to and from public or scientific institutions risks distorting their non-commercial mission. Moreover, often only the incumbent knows the exact value of the interference it experiences, and, likewise, only commercial users can predict accurately the expected monetary outcome from sharing the resource. Thus, our problem is to determine the efficient allocation of resources in the presence of private information without the use of direct monetary transfers. The problem is not unique to spectrum. Other resources that governments hold in trust share the same feature. We propose a novel mechanism design formulation of the problem, characterize the optimal mechanism and describe some of its qualitative properties.",
      "url": "https://arxiv.org/abs/2512.21793v1",
      "pdf_url": "https://arxiv.org/pdf/2512.21793v1",
      "source": "arxiv",
      "source_id": "2512.21793v1",
      "published_date": "2025-12-25",
      "categories": [
        "econ.TH",
        "cs.GT",
        "math.OC"
      ],
      "relevance_score": 8.88888888888889,
      "summary": "This paper addresses the resource allocation problem for radio spectrum sharing between non-commercial incumbents (government, scientific institutions) and commercial users when direct monetary transfers are prohibited and both parties have private information about their valuations. The authors use theoretical mechanism design to characterize the optimal allocation mechanism under these constraints. The key contribution is developing a novel mechanism that achieves efficient spectrum sharing without monetary transfers by leveraging costly inspections and limited transfer instruments, providing a framework applicable to other government-held resources with similar characteristics."
    },
    {
      "title": "Managing Learning Structures",
      "authors": [
        "Hiroto Sato",
        "Ryo Shirakawa"
      ],
      "abstract": "We develop a simple model of a designer who manages a learning structure. Agents have partial private information about a common-value good. The designer wishes to allocate the good to as many agents as possible without using monetary transfers. We formulate this environment as a mechanism design problem that nests social learning models and characterize an optimal mechanism under general distributions over private information. The optimal mechanism can be summarized by two parameters: one purely adjusts the allocation probability, while the other governs the amount of learning implicitly induced by allocation. Although the designer always prefers to allocate the good, managing incentives for learning leads the optimal mechanism to withhold allocation even when allocation is socially efficient. Our analysis brings the perspective of managing learning structures to market design and introduces a mechanism design approach to social learning.",
      "url": "https://arxiv.org/abs/2512.20001v1",
      "pdf_url": "https://arxiv.org/pdf/2512.20001v1",
      "source": "arxiv",
      "source_id": "2512.20001v1",
      "published_date": "2025-12-23",
      "categories": [
        "econ.TH"
      ],
      "relevance_score": 8.88888888888889,
      "summary": "This paper examines how a designer should optimally manage information aggregation when allocating a common-value good to multiple agents without monetary transfers, where agents have partial private information about the good's value. Using mechanism design theory, the authors characterize the optimal allocation mechanism through two key parameters: one controlling allocation probability and another governing learning incentives. The key finding is that the optimal mechanism sometimes withholds socially efficient allocations to maintain proper learning incentives, demonstrating a fundamental trade-off between immediate efficiency and information management in sequential allocation problems."
    },
    {
      "title": "The Algorithmic Barrier: Quantifying Artificial Frictional Unemployment in Automated Recruitment Systems",
      "authors": [
        "Ibrahim Denis Fofanah"
      ],
      "abstract": "The United States labor market exhibits a persistent coexistence of high job vacancy rates and prolonged unemployment duration, a pattern that standard labor market theory struggles to explain. This paper argues that a non-trivial portion of contemporary frictional unemployment is artificially induced by automated recruitment systems that rely on deterministic keyword-based screening.   Drawing on labor economics, information asymmetry theory, and prior work on algorithmic hiring, we formalize this phenomenon as artificial frictional unemployment arising from semantic misinterpretation of candidate competencies. We evaluate this claim using controlled simulations that compare legacy keyword-based screening with semantic matching based on high-dimensional vector representations of resumes and job descriptions.   The results demonstrate substantial improvements in recall and overall matching efficiency without a corresponding loss in precision. Building on these findings, the paper proposes a candidate-side workforce operating architecture that standardizes, verifies, and semantically aligns human capital signals while remaining interoperable with existing recruitment infrastructure. The findings highlight the economic costs of outdated hiring systems and the potential gains from improving semantic alignment in labor market matching.",
      "url": "https://arxiv.org/abs/2601.14534v1",
      "pdf_url": "https://arxiv.org/pdf/2601.14534v1",
      "source": "arxiv",
      "source_id": "2601.14534v1",
      "published_date": "2026-01-20",
      "categories": [
        "cs.CY",
        "econ.GN",
        "math.PR"
      ],
      "relevance_score": 8.88888888888889,
      "summary": "This paper investigates whether automated keyword-based recruitment systems contribute to artificially elevated frictional unemployment by creating semantic mismatches between job seekers and vacancies. Using controlled simulations, the authors compare traditional keyword screening with semantic matching based on high-dimensional vector representations of resumes and job descriptions. The key finding is that semantic matching substantially improves recall and matching efficiency without sacrificing precision, suggesting that outdated algorithmic hiring practices impose significant economic costs on labor market efficiency."
    },
    {
      "title": "Screening for Choice Sets",
      "authors": [
        "Tan Gan",
        "Yingkai Li"
      ],
      "abstract": "We study a screening problem in which an agent privately observes a set of feasible technologies and can strategically disclose only a subset to the principal. The principal then takes an action whose payoff consequences for both players are publicly known. Under the assumption that the possible technology sets are ordered by set inclusion, we show that the optimal mechanism promises the agent a utility that is weakly increasing as the reported set expands, and the choice of the principal maximizes her own utility subject to this promised utility constraint. Moreover, the optimal promised utility either coincides with the agent's utility under the complete information benchmark or remains locally constant, with the number of constant segments bounded by the number of downward-sloping segments of the complete information benchmark.",
      "url": "https://arxiv.org/abs/2601.15580v1",
      "pdf_url": "https://arxiv.org/pdf/2601.15580v1",
      "source": "arxiv",
      "source_id": "2601.15580v1",
      "published_date": "2026-01-22",
      "categories": [
        "econ.TH",
        "cs.GT"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper examines a screening mechanism where an agent privately observes a set of feasible technologies and can strategically disclose only a subset to a principal who then chooses an action. Using theoretical analysis under the assumption that technology sets are ordered by inclusion, the authors show that the optimal mechanism features weakly increasing promised utility as reported sets expand, with the principal maximizing her utility subject to this constraint. The key contribution is proving that optimal promised utility either matches the complete information benchmark or remains locally constant, with constant segments bounded by the downward-sloping segments of the complete information case."
    },
    {
      "title": "From No-Regret to Strategically Robust Learning in Repeated Auctions",
      "authors": [
        "Junyao Zhao"
      ],
      "abstract": "In Bayesian single-item auctions, a monotone bidding strategy--one that prescribes a higher bid for a higher value type--can be equivalently represented as a partition of the quantile space into consecutive intervals corresponding to increasing bids. Kumar et al. (2024) prove that agile online gradient descent (OGD), when used to update a monotone bidding strategy through its quantile representation, is strategically robust in repeated first-price auctions: when all bidders employ agile OGD in this way, the auctioneer's average revenue per round is at most the revenue of Myerson's optimal auction, regardless of how she adjusts the reserve price over time.   In this work, we show that this strategic robustness guarantee is not unique to agile OGD or to the first-price auction: any no-regret learning algorithm, when fed gradient feedback with respect to the quantile representation, is strategically robust, even if the auction format changes every round, provided the format satisfies allocation monotonicity and voluntary participation. In particular, the multiplicative weights update (MWU) algorithm simultaneously achieves the optimal regret guarantee and the best-known strategic robustness guarantee. At a technical level, our results are established via a simple relation that bridges Myerson's auction theory and standard no-regret learning theory. This showcases the potential of translating standard regret guarantees into strategic robustness guarantees for specific games, without explicitly minimizing any form of swap regret.",
      "url": "https://arxiv.org/abs/2601.03853v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03853v1",
      "source": "arxiv",
      "source_id": "2601.03853v1",
      "published_date": "2026-01-07",
      "categories": [
        "cs.GT",
        "cs.LG",
        "econ.TH"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper investigates whether strategic robustness guarantees in repeated auctions extend beyond the specific agile online gradient descent (OGD) algorithm previously studied by Kumar et al. (2024). Using theoretical analysis, the authors establish a general connection between no-regret learning and Myerson's auction theory, proving that any no-regret learning algorithm applied to monotone bidding strategies via quantile representation achieves strategic robustness across auction formats with allocation monotonicity and voluntary participation. The key contribution is demonstrating that strategic robustness is a general property of no-regret learning in this setting rather than algorithm-specific, with multiplicative weights update achieving both optimal regret and best-known strategic robustness guarantees simultaneously."
    },
    {
      "title": "Dynamic Decoupling in Multidimensional Screening",
      "authors": [
        "Eric Gao"
      ],
      "abstract": "I study multidimensional sequential screening. A monopolist contracts with an agent endowed with private information about the distribution of their eventual valuations of different goods; a contract is written and the agent reports their initial private information before drawing and reporting their valuations. In these settings, the monopolist frontloads surplus extraction: Any information rents given to the agent to elicit their post-contractual valuations can be extracted in expectation before valuations are drawn. This significantly simplifies the multidimensional screening problem. If the agent's valuations satisfy invariant dependencies (valuations can be dependent across dimensions, but how valuations are coupled cannot vary in their initial private information), the optimal mechanism coincides with independently offering the optimal sequential screening mechanism for each good, regardless of the dependency structure.",
      "url": "https://arxiv.org/abs/2512.23274v1",
      "pdf_url": "https://arxiv.org/pdf/2512.23274v1",
      "source": "arxiv",
      "source_id": "2512.23274v1",
      "published_date": "2025-12-29",
      "categories": [
        "econ.TH"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper examines how a monopolist optimally designs contracts when an agent has private information about their future valuation distributions across multiple goods. Using theoretical mechanism design, the analysis shows that the monopolist can frontload all information rent extraction before valuations are realized, which dramatically simplifies the multidimensional screening problem. The key finding is that under \"invariant dependencies\" (where correlation structures don't vary with private information), the optimal mechanism decomposes into independent single-good sequential screening mechanisms, making multidimensional complexity irrelevant for optimal design."
    },
    {
      "title": "Calibrated Mechanism Design",
      "authors": [
        "Laura Doval",
        "Alex Smolin"
      ],
      "abstract": "We study mechanism design when a designer repeatedly uses a fixed mechanism to interact with strategic agents who learn from observing their allocations. We introduce a static framework, calibrated mechanism design, requiring mechanisms to remain incentive compatible given the information they reveal about an underlying state through repeated use. In single-agent settings, we prove implementable outcomes correspond to two-stage mechanisms: the designer discloses information about the state, then commits to a state-independent allocation rule. This yields a tractable procedure to characterize calibrated mechanisms, combining information design and mechanism design. In private values environments, full transparency is optimal and correlation-based surplus extraction fails. We provide a microfoundation by showing calibrated mechanisms characterize exactly what is implementable when an infinitely patient agent repeatedly interacts with the same mechanism. Dynamic mechanisms that condition on histories expand implementable outcomes only by weakening incentive compatibility and individual rationality--a distinction that vanishes in transferable utility settings.",
      "url": "https://arxiv.org/abs/2512.17858v1",
      "pdf_url": "https://arxiv.org/pdf/2512.17858v1",
      "source": "arxiv",
      "source_id": "2512.17858v1",
      "published_date": "2025-12-19",
      "categories": [
        "econ.TH"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper investigates how to design mechanisms when strategic agents learn about underlying states from repeated interactions with the same mechanism. The methodology combines theoretical mechanism design with information design, introducing a static \"calibrated mechanism design\" framework that requires mechanisms to remain incentive compatible given the information they reveal through repeated use. The key finding is that implementable outcomes in single-agent settings correspond exactly to two-stage mechanisms where the designer first discloses state information then commits to state-independent allocation rules, with the surprising result that in private values environments, full transparency is optimal and traditional correlation-based surplus extraction mechanisms fail."
    },
    {
      "title": "Multi-agent Adaptive Mechanism Design",
      "authors": [
        "Qiushi Han",
        "David Simchi-Levi",
        "Renfei Tan",
        "Zishuo Zhao"
      ],
      "abstract": "We study a sequential mechanism design problem in which a principal seeks to elicit truthful reports from multiple rational agents while starting with no prior knowledge of agents' beliefs. We introduce Distributionally Robust Adaptive Mechanism (DRAM), a general framework combining insights from both mechanism design and online learning to jointly address truthfulness and cost-optimality. Throughout the sequential game, the mechanism estimates agents' beliefs and iteratively updates a distributionally robust linear program with shrinking ambiguity sets to reduce payments while preserving truthfulness. Our mechanism guarantees truthful reporting with high probability while achieving $\\tilde{O}(\\sqrt{T})$ cumulative regret, and we establish a matching lower bound showing that no truthful adaptive mechanism can asymptotically do better. The framework generalizes to plug-in estimators, supporting structured priors and delayed feedback. To our knowledge, this is the first adaptive mechanism under general settings that maintains truthfulness and achieves optimal regret when incentive constraints are unknown and must be learned.",
      "url": "https://arxiv.org/abs/2512.21794v1",
      "pdf_url": "https://arxiv.org/pdf/2512.21794v1",
      "source": "arxiv",
      "source_id": "2512.21794v1",
      "published_date": "2025-12-25",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "econ.TH"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper addresses sequential mechanism design where a principal must elicit truthful reports from multiple agents without prior knowledge of their belief distributions. The authors develop a theoretical framework called Distributionally Robust Adaptive Mechanism (DRAM) that combines mechanism design with online learning, using estimated agent beliefs to iteratively update a distributionally robust linear program with shrinking ambiguity sets. The key contribution is achieving both truthfulness guarantees and optimal $\\tilde{O}(\\sqrt{T})$ regret bounds (with matching lower bounds), representing the first adaptive mechanism to maintain incentive compatibility while learning unknown agent characteristics in general settings."
    },
    {
      "title": "Incomplete Information and Matching of Likes: A Mechanism Design Approach",
      "authors": [
        "Dinko Dimitrov",
        "Dipjyoti Majumdar"
      ],
      "abstract": "We study the implementability of stable matchings in a two-sided market model with one-sided incomplete information. Firms' types are publicly known, whereas workers' types are private information. A mechanism generates a matching and additional announcements to the firms at each report profile of workers' types. When agents' preferences are increasing in the types of their matched partner, we show that the assortative matching mechanism which publicly announces the entire set of reported types is incentive compatible. Furthermore, any mechanism that limits information disclosure to firms' lower contour sets of reported types remains incentive compatible. However, when information is incomplete on both sides of the market, assortative matching is no longer implementable.",
      "url": "https://arxiv.org/abs/2512.18764v1",
      "pdf_url": "https://arxiv.org/pdf/2512.18764v1",
      "source": "arxiv",
      "source_id": "2512.18764v1",
      "published_date": "2025-12-21",
      "categories": [
        "econ.TH"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper examines whether stable assortative matchings can be implemented when agents have private information about their types in two-sided matching markets. Using mechanism design theory, the authors prove that under one-sided incomplete information (where only workers' types are private), assortative matching remains incentive compatible when the mechanism announces workers' reported types to firms, and this result holds even with limited information disclosure to firms' lower contour sets. However, the key finding is that assortative matching becomes non-implementable when information is incomplete on both sides of the market, highlighting a fundamental limitation of stable matching mechanisms under bilateral private information."
    },
    {
      "title": "Optimal Carbon Prices in an Unequal World: The Role of Regional Welfare Weights",
      "authors": [
        "Simon F. Lang"
      ],
      "abstract": "How should nations price carbon? This paper examines how the treatment of global inequality, captured by regional welfare weights, affects optimal carbon prices. I develop theory to identify the conditions under which accounting for differences in marginal utilities of consumption across countries leads to more stringent global climate policy in the absence of international transfers. I further establish a connection between the optimal uniform carbon prices implied by different welfare weights and heterogeneous regional preferences over climate policy stringency. In calibrated simulations, I find that accounting for global inequality reduces optimal global emissions relative to an inequality-insensitive benchmark. This holds both when carbon prices are regionally differentiated, with emissions 21% lower, and when they are constrained to be globally uniform, with the uniform carbon price 15% higher.",
      "url": "https://arxiv.org/abs/2512.24520v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24520v1",
      "source": "arxiv",
      "source_id": "2512.24520v1",
      "published_date": "2025-12-30",
      "categories": [
        "econ.GN"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper investigates how accounting for global inequality through regional welfare weights affects optimal carbon pricing design. Using theoretical analysis and calibrated simulations, the author examines conditions under which incorporating cross-country differences in marginal utility of consumption leads to more stringent climate policy without international transfers. The key finding is that accounting for global inequality reduces optimal emissions by 21% under regionally differentiated carbon prices and increases uniform global carbon prices by 15% relative to inequality-insensitive benchmarks, demonstrating that distributional concerns systematically favor more aggressive climate policy."
    },
    {
      "title": "Institutional AI: Governing LLM Collusion in Multi-Agent Cournot Markets via Public Governance Graphs",
      "authors": [
        "Marcantonio Bracale Syrnikov",
        "Federico Pierucci",
        "Marcello Galisai",
        "Matteo Prandi",
        "Piercosma Bisconti",
        "Francesco Giarrusso",
        "Olga Sorokoletova",
        "Vincenzo Suriani",
        "Daniele Nardi"
      ],
      "abstract": "Multi-agent LLM ensembles can converge on coordinated, socially harmful equilibria. This paper advances an experimental framework for evaluating Institutional AI, our system-level approach to AI alignment that reframes alignment from preference engineering in agent-space to mechanism design in institution-space. Central to this approach is the governance graph, a public, immutable manifest that declares legal states, transitions, sanctions, and restorative paths; an Oracle/Controller runtime interprets this manifest, attaching enforceable consequences to evidence of coordination while recording a cryptographically keyed, append-only governance log for audit and provenance. We apply the Institutional AI framework to govern the Cournot collusion case documented by prior work and compare three regimes: Ungoverned (baseline incentives from the structure of the Cournot market), Constitutional (a prompt-only policy-as-prompt prohibition implemented as a fixed written anti-collusion constitution, and Institutional (governance-graph-based). Across six model configurations including cross-provider pairs (N=90 runs/condition), the Institutional regime produces large reductions in collusion: mean tier falls from 3.1 to 1.8 (Cohen's d=1.28), and severe-collusion incidence drops from 50% to 5.6%. The prompt-only Constitutional baseline yields no reliable improvement, illustrating that declarative prohibitions do not bind under optimisation pressure. These results suggest that multi-agent alignment may benefit from being framed as an institutional design problem, where governance graphs can provide a tractable abstraction for alignment-relevant collective behavior.",
      "url": "https://arxiv.org/abs/2601.11369v2",
      "pdf_url": "https://arxiv.org/pdf/2601.11369v2",
      "source": "arxiv",
      "source_id": "2601.11369v2",
      "published_date": "2026-01-16",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper investigates how to prevent collusion among multiple LLM agents competing in Cournot markets through institutional governance mechanisms rather than prompt engineering. The authors use an experimental methodology comparing three regimes across 90 runs per condition: ungoverned baseline, prompt-based constitutional rules, and their \"Institutional AI\" system that employs governance graphs with enforceable sanctions via an Oracle/Controller runtime. The key finding is that governance-graph-based institutional mechanisms dramatically reduce collusion (mean collusion tier from 3.1 to 1.8, severe collusion from 50% to 5.6%), while prompt-only constitutional approaches provide no reliable improvement, demonstrating that enforceable institutional design outperforms declarative prohibitions under optimization pressure."
    },
    {
      "title": "Mechanism Design for Federated Learning with Non-Monotonic Network Effects",
      "authors": [
        "Xiang Li",
        "Bing Luo",
        "Jianwei Huang",
        "Yuan Luo"
      ],
      "abstract": "Mechanism design is pivotal to federated learning (FL) for maximizing social welfare by coordinating self-interested clients. Existing mechanisms, however, often overlook the network effects of client participation and the diverse model performance requirements (i.e., generalization error) across applications, leading to suboptimal incentives and social welfare, or even inapplicability in real deployments. To address this gap, we explore incentive mechanism design for FL with network effects and application-specific requirements of model performance. We develop a theoretical model to quantify the impact of network effects on heterogeneous client participation, revealing the non-monotonic nature of such effects. Based on these insights, we propose a Model Trading and Sharing (MoTS) framework, which enables clients to obtain FL models through either participation or purchase. To further address clients' strategic behaviors, we design a Social Welfare maximization with Application-aware and Network effects (SWAN) mechanism, exploiting model customer payments for incentivization. Experimental results on a hardware prototype demonstrate that our SWAN mechanism outperforms existing FL mechanisms, improving social welfare by up to $352.42\\%$ and reducing extra incentive costs by $93.07\\%$.",
      "url": "https://arxiv.org/abs/2601.04648v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04648v1",
      "source": "arxiv",
      "source_id": "2601.04648v1",
      "published_date": "2026-01-08",
      "categories": [
        "cs.GT",
        "cs.DC",
        "cs.LG"
      ],
      "relevance_score": 6.666666666666667,
      "summary": "This paper investigates how to design optimal incentive mechanisms for federated learning when client participation creates non-monotonic network effects and applications have heterogeneous model performance requirements. The authors develop a theoretical framework quantifying these network effects and propose the SWAN mechanism within a Model Trading and Sharing (MoTS) framework that allows clients to either participate in training or purchase models directly. The mechanism achieves up to 352% improvement in social welfare and 93% reduction in incentive costs compared to existing approaches by strategically exploiting payments from model purchasers to subsidize participant incentives."
    }
  ]
}